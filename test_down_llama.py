
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œí•œ LLaMA ëª¨ë¸ ê²½ë¡œ
local_dir = r"C:\han\models\llama-3.1-8b"

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tok = AutoTokenizer.from_pretrained(local_dir)
model = AutoModelForCausalLM.from_pretrained(
    local_dir,
    device_map="auto",
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    # load_in_4bit=True,  # (ì˜µì…˜) VRAM ì ìœ¼ë©´ bitsandbytes ì„¤ì¹˜ í›„ í™œì„±í™”
)

# ëŒ€í™” ì´ë ¥ ì €ìž¥
history = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•œêµ­ì–´ ì¡°ìˆ˜ìž…ë‹ˆë‹¤."}
]

def chat(messages, max_new_tokens=200):
    """ëŒ€í™” ì´ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tok(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
        )
    return tok.decode(out[0], skip_special_tokens=True)

if __name__ == "__main__":
    print("ðŸ’¬ LLaMA ëŒ€í™” ì‹œìž‘ (ì¢…ë£Œ: Ctrl+C)")
    try:
        while True:
            user_msg = input("\nðŸ‘¤ ì‚¬ìš©ìž> ").strip()
            if not user_msg:
                continue
            history.append({"role": "user", "content": user_msg})
            answer = chat(history)
            print(f"ðŸ¤– LLaMA> {answer}")
            history.append({"role": "assistant", "content": answer})
    except (KeyboardInterrupt, EOFError):
        print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
